{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYo4UaRStTh_"
   },
   "source": [
    "# [IMPORTANT!] EXPERIMENTAL\n",
    "\n",
    "\n",
    "Text2SQL V1 is an experimental solution that combines calls to Vertex LLM with BQ executions. It is important to note that executing the generated SQL query could potentially modify the data and shouldn't be run on production datasets.\n",
    "\n",
    "Experiments are focused on validating a prototype and are not guaranteed to be released. Experiments are covered by the Pre-GA Offerings Terms of the Google Cloud Platform Terms of Service. They are not intended for production use or covered by any SLA, support obligation, or deprecation policy and might be subject to backward-incompatible changes.\n",
    "\n",
    "The underlying model/API is GA with billing enabled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HIco9ZZMR92"
   },
   "source": [
    "# Generative AI: Text2SQL V1 Instructions [External]\n",
    "\n",
    "---\n",
    "This notebook demonstrates the Experimental Text2SQL V1 capabilities offering on Vertex AI.\n",
    "\n",
    "Please note that Text2SQL V1 is in the early stages of development. The GCP project does not require allowlisting.\n",
    "\n",
    "---\n",
    "This notebook is composed of 2 parts.\n",
    "1. Deployment of Vertex AI Model and Vertex AI Endpoint on the user project\n",
    "2. Text2SQL using the endpoint calls to the deployed endpoint\n",
    "\n",
    "Requirements:\n",
    "- google-cloud-aiplatform>=1.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_ACNUVCBSEL",
    "outputId": "23634280-a97d-41e9-e5c1-30b5b00e4406",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_3qJD2iBCIz",
    "outputId": "f25e4d9a-3e5f-4a13-ba58-e7adea3fae72"
   },
   "outputs": [],
   "source": [
    "# @title # Authentication. { display-mode: \"form\"}\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jn5uzm3n2ypg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/896267025569/locations/us-central1/models/2454538762730864640/operations/1235255890232213504\n",
      "Model created. Resource name: projects/896267025569/locations/us-central1/models/2454538762730864640@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/896267025569/locations/us-central1/models/2454538762730864640@1')\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/896267025569/locations/us-central1/endpoints/5400240364705546240/operations/7227998881890435072\n"
     ]
    }
   ],
   "source": [
    "# @title # Deploying Endpoint to Project\n",
    "# @markdown We provide a custom container **us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/text2sql_bq** which can be deployed to your project directly. This is a one time action as long as there is a deployed endpoint.\n",
    "\n",
    "# @markdown Specify the following parameters for the endpoint deployment\n",
    "MODEL_DISPLAY_NAME = \"text2sql-model\"\n",
    "IMAGE_URI = \"us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/text2sql_bq\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "PROJECT_ID=\"wmt-7fbls2a91f025anb93e025b02g\" # @param {type:\"string\"}\n",
    "MACHINE_TYPE = \"e2-standard-2\" # @param {type:\"string\"}\n",
    "# @markdown Furthermore, we need a service account that the DeployedModel's container runs as. Please give \"BigQuery Data Viewer\", \"Vertex AI Service Agent\" and \"Vertex AI User\" permissions in IAM for the service account used to run the Text2SQL pipeline\n",
    "\n",
    "# @markdown   Example: text2sqldemo@your-project.iam.gserviceaccount.com. You can refer [here](https://cloud.google.com/iam/docs/service-accounts-create) for more information.\n",
    "SERVICE_ACCOUNT = \"svc-deploy-mgmt@wmt-7fbls2a91f025anb93e025b02g.iam.gserviceaccount.com\" # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "def upload_model():\n",
    "  model = aiplatform.Model.upload(\n",
    "      display_name=MODEL_DISPLAY_NAME,\n",
    "      serving_container_image_uri=IMAGE_URI,\n",
    "  )\n",
    "  return model\n",
    "\n",
    "def deploy_model_to_endpoint(deployed_model):\n",
    "  endpoint = deployed_model.deploy(\n",
    "    machine_type=MACHINE_TYPE, service_account=SERVICE_ACCOUNT\n",
    "  )\n",
    "  return endpoint\n",
    "\n",
    "def undeploy_model_and_delete_endpoint(endpoint_id):\n",
    "  endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name=f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\")\n",
    "\n",
    "  endpoint.undeploy_all()\n",
    "  endpoint.delete()\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Endpoint deployment\n",
    "model = upload_model()\n",
    "endpoint = deploy_model_to_endpoint(model)\n",
    "print(f'Endpoint id is: {endpoint.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugshG0Y6AhO_"
   },
   "outputs": [],
   "source": [
    "# @title # Running Text2SQL. { display-mode: \"form\" }\n",
    "\n",
    "# Variables added if a user has already deployed the endpoint\n",
    "LOCATION = \"us-central1\"\n",
    "PROJECT_ID=\"wmt-7fbls2a91f025anb93e025b02g\" # @param {type:\"string\"}\n",
    "\n",
    "# @markdown Once you have deployed the endpoint (check previous section) with the Text2SQL server, specify the following parameters. For more information, check the [endpoint documentation](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints). Endpoint deployment might take a few minutes to run.\n",
    "ENDPOINT_ID = \"5400240364705546240\" #@param {type: \"string\"}\n",
    "\n",
    "# @markdown Now we will build the Text2SQL endpoint request which will called to convert the natural language english question into a BQ SQL query.\n",
    "# @markdown Questions should not contain any instructions related to modifying the table such as \"delete\", \"change\", \"drop\", \"remove\", \"update\" and only contain language related to querying the data such as \"What\" and \"how many\".\n",
    "NL_QUESTION = \"Get total number of people from state of WA grouped by name and state, limit results by 10 and sort by highest to lowest.\" # @param {type:\"string\"}\n",
    "BQ_DATASET = \"bigquery-public-data.usa_names\" # @param {type:\"string\"}\n",
    "\n",
    "# @markdown ## Important!\n",
    "# @markdown When the BQ dataset is too large, the underlying prompt might be larger than the supported token limit of the LLM, therefore we provide a way to provide filters that limit tables/columns from the schema that the model will have access to.\n",
    "# @markdown For more information on LLM token limits, check [here](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models).\n",
    "\n",
    "# @markdown If the Text2SQL endpoint requests fails with an error similar to `[ORIGINAL ERROR] generic::invalid_argument: The model supports up to 8192 input tokens, but received 2100001 tokens.`, specify the parameters below.\n",
    "\n",
    "# @markdown The following are optional parameters but help mitigate the issue with the token limit:\n",
    "\n",
    "# @markdown 1. List of strings of BQ table names denoting subset from the BQ dataset that should be considered. Use the full table name, e.g. **[\"bigquery-public-data.london_bicycles.cycle_stations\", ...]**.\n",
    "BQ_TABLES_FILTER = [] #@param {type:\"raw\"}\n",
    "# @markdown 2. Optional dict of { table name strings : list of columns names } denoting columns to be considered for each table. The model will only see the table bigquery-public-data.london_bicycles.cycle_stations and think that it only has 3 columns:  \"id\", \"name\", \"install_date\". Should only be used when execution_mode=\"manual\". E.g. **{ \"bigquery-public-data.london_bicycles.cycle_stations\": [\"id\", \"name\", \"install_date\"] }**\n",
    "BQ_COLUMNS_FILTER = {} #@param {type:\"raw\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Nfl_XyhZOKsO"
   },
   "outputs": [],
   "source": [
    "# @title ### Helper: building the request\n",
    "\n",
    "# @markdown We can control the number of samples made to the LLM which improves the chance of the generated sql query to be correct.\n",
    "# @markdown A lower value translated to lower latency but worse performance. Additionaly, the request cost will be proportional to `NB_SAMPLES`.\n",
    "NB_SAMPLES = 5 # @param {type:\"integer\"}\n",
    "TEMPERATURE = 0.8 # @param {type:\"number\"}\n",
    "\n",
    "def build_request(bq_tables_filter = None, bq_columns_filter = None):\n",
    "  \"\"\"Creates a single endpoint request.\"\"\"\n",
    "\n",
    "  example_input = [\n",
    "    {\n",
    "      \"text\": NL_QUESTION,\n",
    "      \"bq_dataset\": BQ_DATASET,\n",
    "      \"project_id\": PROJECT_ID,\n",
    "    }]\n",
    "  if bq_tables_filter:\n",
    "    example_input[0][\"bq_tables_filter\"] = bq_tables_filter\n",
    "  if bq_columns_filter:\n",
    "    example_input[0][\"bq_columns_filter\"] = bq_columns_filter\n",
    "\n",
    "  params = {\n",
    "    \"temperature\":0.8,\n",
    "    \"nb_samples\":NB_SAMPLES,\n",
    "  }\n",
    "\n",
    "  return example_input, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zl9c22rzDoHj",
    "outputId": "b82629c5-64eb-410d-b4b5-221c87d282c5"
   },
   "outputs": [],
   "source": [
    "# @title ### Call Text2SQL endpoint\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# build request\n",
    "example_input, params = build_request(bq_columns_filter=BQ_COLUMNS_FILTER, bq_tables_filter=BQ_TABLES_FILTER)\n",
    "\n",
    "# Query via Endpoint directly\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name=f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\")\n",
    "\n",
    "# The endpoint request\n",
    "response = endpoint.predict(instances=example_input, parameters=params)\n",
    "\n",
    "# response.predictions[0] is a dictionary:\n",
    "# {\n",
    "#  \"status\" \"result\" or \"exception\",\n",
    "#  \"sql_query\" : \"Generated SQL query string\"\n",
    "# }\n",
    "response.predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwJx7nG2eQYh",
    "outputId": "12cb677a-2d1d-4486-bc75-1cb2d8795ea0"
   },
   "outputs": [],
   "source": [
    "sql_query = response.predictions[0]['sql_query']\n",
    "print(\"Generated SQL query:\", sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "RPXbgAsaFBTU",
    "outputId": "67c0d3c1-38b7-4aad-a9db-17cd9051f1bc"
   },
   "outputs": [],
   "source": [
    "#@title [Optional] Execute generated SQL query on BQ data\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "df_res = client.query(sql_query).to_dataframe()\n",
    "\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pPRDo4lw6RL4"
   },
   "outputs": [],
   "source": [
    "#@title [Optional] Cleaning up\n",
    "\n",
    "# @markdown When you are done, undeploy and delete the endpoint. Uncomment the following code.\n",
    "# undeploy_model_and_delete_endpoint(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_yMQyJKUwRd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "conda-env-genai-py",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "Python 3 (genai)",
   "language": "python",
   "name": "conda-env-genai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
