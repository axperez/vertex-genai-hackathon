{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxCkB_DXTHzf"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hny4I-ODTIS6"
   },
   "source": [
    "# Text Summarization of Large Documents using LangChain ðŸ¦œðŸ”—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nLS57E2TO5y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Text summarization is an NLP task that creates a concise and informative summary of a longer text. LLMs can be used to create summaries of news articles, research papers, technical documents, and other types of text.\n",
    "\n",
    "Summarizing large documents can be challenging. To create summaries, you need to apply summarization strategies to your indexed documents. You have already seen some of these strategies in the previous notebooks. If you haven't completed it, it is recommended to do so to have a basic understanding of how to summarize large documents.\n",
    "\n",
    "In this notebook, you will use LangChain, a framework for developing LLM applications, to apply some summarization strategies. The notebook covers several examples of how to summarize large documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXsvgIuwTPZw"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use LangChain with PaLM API to summarize large documents by working through the following examples:\n",
    "\n",
    "- Stuffing method\n",
    "- MapReduce method\n",
    "- Refine method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skXAu__iqks_"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI Generative AI Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvKl-BtQTRiQ"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwFMpIMrTV_4"
   },
   "source": [
    "### SKIP: Install Vertex AI SDK & Other dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip pip install for hackathon since genai conda env has all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aP6JVlZkS-m",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !sudo apt -y -qq install tesseract-ocr\n",
    "# !sudo apt -y -qq install libtesseract-dev\n",
    "# !sudo apt-get -y -qq install poppler-utils #required by PyPDF2 for page count and other pdf utilities\n",
    "# !sudo apt-get -y -qq install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDmNq5__Trl4"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     USER = \"--user\"\n",
    "# else:\n",
    "#     USER = \"\"\n",
    "\n",
    "# ! pip3 install {USER} --upgrade pytesseract pypdf PyPDF2 textract langchain transformers google-cloud-aiplatform --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell when not on Vertex AI Workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjSsu6cmUdEx"
   },
   "outputs": [],
   "source": [
    "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "# REGION = \"us-central1\"\n",
    "\n",
    "# import vertexai\n",
    "\n",
    "# vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5fXfvzhTkYN"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cRkcfnQMT9vD"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAGaTjPVTmhP"
   },
   "source": [
    "### Import models\n",
    "\n",
    "You load the pre-trained text generation model called `text-bison@001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ITUmZiNZcMUW"
   },
   "outputs": [],
   "source": [
    "vertex_llm_text = VertexAI(model_name=\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKG-ZTJ_02wq"
   },
   "source": [
    "## Summarization with Large Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZkLDRTjTcfm"
   },
   "source": [
    "### Preparing data files\n",
    "\n",
    "To begin, you will need to download a few files that are required for the summarizing tasks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7H0zINHpTaSu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/jupyter/vertex-genai-hackathon/use-cases/document-summarization/data/practitioners_guide_to_mlops_whitepaper.pdf',\n",
       " <http.client.HTTPMessage at 0x7f9ce7f89cf0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = p.cwd() / \"data\"\n",
    "p(data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_url = \"https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf\"\n",
    "pdf_file = str(p(data_folder, pdf_url.split(\"/\")[-1]))\n",
    "\n",
    "urllib.request.urlretrieve(pdf_url, pdf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JELITHdBhnf0"
   },
   "source": [
    "###Â Extract text from the PDF\n",
    "\n",
    "You use an `PdfReader` to extract the text from our scanned documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x3INtovxreI_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Organizations can use the framework to identify gaps in building an integrated ML platform and to focus on the scale \n",
      "and automate themes from Googleâ€™s AI Adoption Framework. The decision about whether (or to which degree) to \n",
      "adopt each of these processes and capabilities in your organization depends on your business context. For exam -\n",
      "ple, you must determine the business value that the framework creates when compared to the cost of purchasing or \n",
      "building capabilities (for example, the cost in engineering hours).\n",
      "Overview of MLOps lifecycle  \n",
      "and core capabilities\n",
      "Despite the growing recognition of AI/ML as a crucial pillar of digital transformation, successful deployments and \n",
      "effective operations are a bottleneck for getting value from AI. Only one in two organizations has moved beyond \n",
      "pilots and proofs of concept. Moreover, 72% of a cohort of organizations that began AI pilots before 2019 have not \n",
      "been able to deploy even a single application in production.1 Algorithmiaâ€™s survey of the state of enterprise machine \n",
      "learning found that 55% of companies surveyed have not deployed an ML model.2 To summarize: models donâ€™t make \n",
      "it into production, and if they do, they break because they fail to adapt to changes in the environment.\n",
      "This is due to a variety of issues. Teams engage in a high degree of manual and one-off work. They do not have reus -\n",
      "able or reproducible components, and their processes involve difficulties in handoffs between data scientists and IT. \n",
      "Deloitte identified lack of talent and integration issues as factors that can stall or derail AI initiatives.3 Algorithmiaâ€™s \n",
      "survey highlighted that challenges in deployment, scaling, and versioning efforts still hinder teams from getting value \n",
      "from their investments in ML. Capgemini Research noted that the top three challenges faced by organizations in \n",
      "achieving deployments at scale are lack of mid- to senior-level talent, lack of change-management processes, and \n",
      "lack of strong governance models for achieving scale.\n",
      "The common theme in these and other studies is that ML systems cannot be built in an ad hoc manner, isolated from \n",
      "other IT initiatives like DataOps and DevOps. They also cannot be built without adopting and applying sound software \n",
      "engineering practices, while taking into account the factors that make operationalizing ML different from operational -\n",
      "izing other types of software.\n",
      "Organizations need an automated and streamlined ML process. This process does not just help the organization \n",
      "successfully deploy ML models in production. It also helps manage risk when organizations scale the number of \n",
      "ML applications to more use cases in changing environments, and it helps ensure that the applications are still in \n",
      "line with business goals. McKinseyâ€™s Global Survey on AI found that having standard frameworks and development \n",
      "1 The AI-powered enterprise , CapGemini Research Institute, 2020.\n",
      "2 2020 state of enterprise machine learning , Algorithmia, 2020.\n",
      "3 Artificial intelligence for the real world , Deloitte, 2017.\n",
      "4 The state of AI in 2020 , McKinsey, 2020.\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(pdf_file)\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(pages[3].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDVwBFSjZ7ws"
   },
   "source": [
    "## Method 1: Stuffing\n",
    "\n",
    "Stuffing is the simplest method to pass data to a language model. It \"stuffs\" text into the prompt as context in a way that all of the relevant information can be processed by the model to get what you want.\n",
    "\n",
    "In LangChain, you can use `StuffDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is setting `stuff` as `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhEi-XqKnv2v"
   },
   "source": [
    "### Prompt design with `Stuffing` chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "B-ljajUen1YO"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "  \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aVrDWkJs3Y"
   },
   "source": [
    "### Retrying\n",
    "Initiate a chain using `stuff` method and process three pages document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "N_hoizIgObe9"
   },
   "outputs": [],
   "source": [
    "stuff_chain = load_summarize_chain(vertex_llm_text, chain_type=\"stuff\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q1_zwxwgTnlV"
   },
   "outputs": [],
   "source": [
    "three_pages = pages[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0jEUfOn7UFI2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Practitioners guide to MLOps:  \\nA framework for continuous \\ndelivery and automation of  \\nmachine learning.White paper\\nMay 2021\\nAuthors:  \\nKhalid Salama,  \\nJarek Kazmierczak,  \\nDonna Schut', metadata={'source': '/home/jupyter/vertex-genai-hackathon/use-cases/document-summarization/data/practitioners_guide_to_mlops_whitepaper.pdf', 'page': 0}),\n",
       " Document(page_content='Table of Contents\\nExecutive summary  3\\nOverview of MLOps lifecycle and core capabilities  4\\nDeep dive of MLOps processes  15\\nPutting it all together  34\\nAdditional resources  36Building an ML-enabled system  6\\nThe MLOps lifecycle  7\\nMLOps: An end-to-end workflow  8\\nMLOps capabilities  9\\n      Experimentation  11\\n      Data processing  11\\n      Model training  11\\n      Model evaluation  12\\n      Model serving  12\\n      Online experimentation  13\\n      Model monitoring  13\\n      ML pipelines  13\\n      Model registry  14\\n      Dataset and feature repository  14\\n      ML metadata and artifact tracking  15\\nML development  16\\nTraining operationalization  18\\nContinuous training  20\\nModel deployment  23\\nPrediction serving  25\\nContinuous monitoring  26\\nData and model management  29\\n      Dataset and feature management  29\\n             Feature management  30\\n             Dataset management  31\\n      Model management  32\\n             ML metadata tracking  32\\n             Model governance  33', metadata={'source': '/home/jupyter/vertex-genai-hackathon/use-cases/document-summarization/data/practitioners_guide_to_mlops_whitepaper.pdf', 'page': 1}),\n",
       " Document(page_content='Executive summary\\nAcross industries, DevOps and DataOps have been widely adopted as methodologies to improve quality and re -\\nduce the time to market of software engineering and data engineering initiatives. With the rapid growth in machine \\nlearning (ML) systems, similar approaches need to be developed in the context of ML engineering, which handle the \\nunique complexities of the practical applications of ML. This is the domain of MLOps. MLOps is a set of standard -\\nized processes and technology capabilities for building, deploying, and operationalizing ML systems rapidly and \\nreliably.]\\nWe previously published Google Cloudâ€™s AI Adoption Framework  to provide guidance for technology leaders who \\nwant to build an effective artificial intelligence (AI) capability in order to transform their business. That framework \\ncovers AI challenges around people, data, technology, and process, structured in six different themes: learn, lead, \\naccess, secure, scale, and automate . \\nThe current document takes a deeper dive into the themes of scale  and automate  to illustrate the requirements for \\nbuilding and operationalizing ML systems. Scale  concerns the extent to which you use cloud managed ML services \\nthat scale with large amounts of data and large numbers of data processing and ML jobs, with reduced operational \\noverhead. Automate  concerns the extent to which you are able to deploy, execute, and operate technology for data \\nprocessing and ML pipelines in production efficiently, frequently, and reliably.\\nWe outline an MLOps framework that defines core processes and technical capabilities. Organizations can use this \\nframework to help establish mature MLOps practices for building and operationalizing ML systems. Adopting the \\nframework can help organizations improve collaboration between teams, improve the reliability and scalability of ML \\nsystems, and shorten development cycle times. These benefits in turn drive innovation and help gain overall busi -\\nness value from investments in ML.\\nThis document is intended for technology leaders and enterprise architects who want to understand MLOps. Itâ€™s also \\nfor teams who want details about what MLOps looks like in practice. The document assumes that readers are famil -\\niar with basic machine learning concepts and with development and deployment practices such as CI/CD.\\nThe document is in two parts. The first part, an overview of the MLOps lifecycle, is for all readers. It introduces \\nMLOps processes and capabilities and why theyâ€™re important for successful adoption of ML-based systems.\\nThe second part is a deep dive on the MLOps processes and capabilities. This part is for readers who want to un -\\nderstand the concrete details of tasks like running a continuous training pipeline, deploying a model, and monitoring \\npredictive performance of an ML model.3', metadata={'source': '/home/jupyter/vertex-genai-hackathon/use-cases/document-summarization/data/practitioners_guide_to_mlops_whitepaper.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QnXUwWxkrLu4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- MLOps is a set of standardized processes and technology capabilities for building, deploying, and operationalizing ML systems rapidly and reliably.\n",
      "   - MLOps is important for successful adoption of ML-based systems because it can help organizations improve collaboration between teams, improve the reliability and scalability of ML systems, and shorten development cycle times.\n",
      "   - The MLOps lifecycle consists of five stages: ML development, training operationalization, continuous training, model deployment, and prediction serving.\n",
      "   - MLOps capabilities include experimentation, data processing, model training, model evaluation, model serving, online experimentation, model monitoring, ML pipelines,\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(stuff_chain.run(three_pages))\n",
    "except Exception as e:\n",
    "    print(\n",
    "        \"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",\n",
    "        e,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKb_fBEedZqu"
   },
   "source": [
    "As you can see, with the `stuff` method, you can summarize the entire document content with a single API call passing all data at once.\n",
    "\n",
    "Depending on the context length of LLM, the `stuff` method would not work as it result in a prompt larger than the context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RtgemmBzkddX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code failed since it won't be able to run inference on such a huge context and throws this exception:  400 The model supports up to 8192 input tokens, but received 13326 tokens.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(stuff_chain.run(pages))\n",
    "except Exception as e:\n",
    "    print(\n",
    "        \"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",\n",
    "        e,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vtp21WX3T7d_"
   },
   "source": [
    "As expected, the code returns the expection message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqZrKM32h-o2"
   },
   "source": [
    "### Considerations\n",
    "\n",
    "The `stuffing` method is a way to summarize text by feeding the entire document to a large language model (LLM) in a single call. This method has both pros and cons.\n",
    "\n",
    "The stuffing method only requires a single call to the LLM, which can be faster than other methods that require multiple calls. When summarizing text, the LLM has access to all the data at once, which can result in a better summary.\n",
    "\n",
    "But, LLMs have a context length, which is the maximum number of tokens that can be processed in a single call. If the document is longer than the context length, the stuffing method will not work. Also the stuffing method is not suitable for summarizing large documents, as it can be slow and may not produce a good summary.\n",
    "\n",
    "Let's explore other approaches to help deal with having longer text than context lengh limit of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM3V1JARZ9-k"
   },
   "source": [
    "## Method 2: MapReduce\n",
    "\n",
    "The `MapReduce` method implements a multi-stage summarization. It is a technique for summarizing large pieces of text by first summarizing smaller chunks of text and then combining those summaries into a single summary.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is setting `map_reduce` as `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lagLXEamlPY2"
   },
   "source": [
    "### Prompt design with `MapReduce` chain\n",
    "\n",
    "In our example, you have a 32-page document that you need to summarize.\n",
    "\n",
    "With LangChain, the `map_reduce` chain breaks the document down into 1024 token chunks max. Then it runs the initial prompt you define on each chunk to generate a summary of that chunk. In the example below, you use the following first stage or map prompt.\n",
    "\n",
    "```Write a concise summary of the following text delimited by triple backquotes. Return your response in bullet points which covers the key points of the text.\n",
    "'''{text}'''. BULLET POINT SUMMARY:```\n",
    "\n",
    "Once summaries for all of the chunks are generated, it runs a different prompt to combine those summaries into a single summary. In the example below, you use the following second stage or combine prompt.\n",
    "\n",
    "```Write a summary of the entire document that includes the main points from all of the individual summaries.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "R6oHEtdSmsTn"
   },
   "outputs": [],
   "source": [
    "map_prompt_template = \"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "                      Write a concise summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXoz0uLDMoWD"
   },
   "source": [
    "### Generate summaries using MapReduce method\n",
    "\n",
    "After defining prompts, you initialize the associated `map_reduce_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VRGJcBZeVdEa"
   },
   "outputs": [],
   "source": [
    "map_reduce_chain = load_summarize_chain(\n",
    "    vertex_llm_text,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6fekDDr0hrJ"
   },
   "source": [
    "Then, you generate summaries using the chain. Notice that Langchain use a tokenizer (from transformer library) with 1024 token limit by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uSC6w2TBV35q",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4084 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1115 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1246 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1248 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1382 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1454 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1516 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1646 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1783 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1915 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2122 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2252 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2382 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2512 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2558 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2591 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2723 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2854 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2989 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3121 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1077 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1212 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1248 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "map_reduce_outputs = map_reduce_chain({\"input_documents\": pages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meH2ELuz2H46"
   },
   "source": [
    "After summaries are generated, you can validate them by organize input documents and associated output in a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "r6FRSR7xRLew"
   },
   "outputs": [],
   "source": [
    "final_mp_data = []\n",
    "for doc, out in zip(\n",
    "    map_reduce_outputs[\"input_documents\"], map_reduce_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_mp_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dA9cnh8YaNbF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunks</th>\n",
       "      <th>concise_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Practitioners guide to MLOps:  \\nA framework f...</td>\n",
       "      <td>This white paper provides a guide for practiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>Table of Contents\\nExecutive summary  3\\nOverv...</td>\n",
       "      <td>This document provides an overview of MLOps, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>Executive summary\\nAcross industries, DevOps a...</td>\n",
       "      <td>This document provides an overview of MLOps, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>4\\nOrganizations can use the framework to iden...</td>\n",
       "      <td>- MLOps is a framework that helps organization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>5\\nprocesses in place is one of the differenti...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 file_name file_type  page_number  \\\n",
       "0  practitioners_guide_to_mlops_whitepaper      .pdf            0   \n",
       "1  practitioners_guide_to_mlops_whitepaper      .pdf            1   \n",
       "2  practitioners_guide_to_mlops_whitepaper      .pdf            2   \n",
       "3  practitioners_guide_to_mlops_whitepaper      .pdf            3   \n",
       "4  practitioners_guide_to_mlops_whitepaper      .pdf            4   \n",
       "\n",
       "                                              chunks  \\\n",
       "0  Practitioners guide to MLOps:  \\nA framework f...   \n",
       "1  Table of Contents\\nExecutive summary  3\\nOverv...   \n",
       "2  Executive summary\\nAcross industries, DevOps a...   \n",
       "3  4\\nOrganizations can use the framework to iden...   \n",
       "4  5\\nprocesses in place is one of the differenti...   \n",
       "\n",
       "                                     concise_summary  \n",
       "0  This white paper provides a guide for practiti...  \n",
       "1  This document provides an overview of MLOps, i...  \n",
       "2  This document provides an overview of MLOps, w...  \n",
       "3  - MLOps is a framework that helps organization...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_mp_summary = pd.DataFrame.from_dict(final_mp_data)\n",
    "pdf_mp_summary = pdf_mp_summary.sort_values(\n",
    "    by=[\"file_name\", \"page_number\"]\n",
    ")  # sorting the dataframe by filename and page_number\n",
    "pdf_mp_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_mp_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yA0eM1K3cvH2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Context]\n",
      "4\n",
      "Organizations can use the framework to identify gaps in building an integrated ML platform and to focus on the scale \n",
      "and automate themes from Googleâ€™s AI Adoption Framework. The decision about whether (or to which degree) to \n",
      "adopt each of these processes and capabilities in your organization depends on your business context. For exam -\n",
      "ple, you must determine the business value that the framework creates when compared to the cost of purchasing or \n",
      "building capabilities (for example, the cost in engineering hours).\n",
      "Overview of MLOps lifecycle  \n",
      "and core capabilities\n",
      "Despite the growing recognition of AI/ML as a crucial pillar of digital transformation, successful deployments and \n",
      "effective operations are a bottleneck for getting value from AI. Only one in two organizations has moved beyond \n",
      "pilots and proofs of concept. Moreover, 72% of a cohort of organizations that began AI pilots before 2019 have not \n",
      "been able to deploy even a single application in production.1 Algorithmiaâ€™s survey of the state of enterprise machine \n",
      "learning found that 55% of companies surveyed have not deployed an ML model.2 To summarize: models donâ€™t make \n",
      "it into production, and if they do, they break because they fail to adapt to changes in the environment.\n",
      "This is due to a variety of issues. Teams engage in a high degree of manual and one-off work. They do not have reus -\n",
      "able or reproducible components, and their processes involve difficulties in handoffs between data scientists and IT. \n",
      "Deloitte identified lack of talent and integration issues as factors that can stall or derail AI initiatives.3 Algorithmiaâ€™s \n",
      "survey highlighted that challenges in deployment, scaling, and versioning efforts still hinder teams from getting value \n",
      "from their investments in ML. Capgemini Research noted that the top three challenges faced by organizations in \n",
      "achieving deployments at scale are lack of mid- to senior-level talent, lack of change-management processes, and \n",
      "lack of strong governance models for achieving scale.\n",
      "The common theme in these and other studies is that ML systems cannot be built in an ad hoc manner, isolated from \n",
      "other IT initiatives like DataOps and DevOps. They also cannot be built without adopting and applying sound software \n",
      "engineering practices, while taking into account the factors that make operationalizing ML different from operational -\n",
      "izing other types of software.\n",
      "Organizations need an automated and streamlined ML process. This process does not just help the organization \n",
      "successfully deploy ML models in production. It also helps manage risk when organizations scale the number of \n",
      "ML applications to more use cases in changing environments, and it helps ensure that the applications are still in \n",
      "line with business goals. McKinseyâ€™s Global Survey on AI found that having standard frameworks and development \n",
      "1 The AI-powered enterprise , CapGemini Research Institute, 2020.\n",
      "2 2020 state of enterprise machine learning , Algorithmia, 2020.\n",
      "3 Artificial intelligence for the real world , Deloitte, 2017.\n",
      "4 The state of AI in 2020 , McKinsey, 2020.\n",
      "\n",
      "\n",
      " [Simple Summary]\n",
      "- MLOps is a framework that helps organizations operationalize machine learning.\n",
      "                      - MLOps helps organizations to successfully deploy ML models in production, manage risk when scaling the number of ML applications, and ensure that the applications are still in line with business goals.\n",
      "                      - MLOps is made up of a set of processes and capabilities that cover the entire ML lifecycle, from data preparation to model deployment and monitoring.\n",
      "                      - The key processes and capabilities of MLOps include:\n",
      "                        - Data preparation: This involves cleaning and preparing data for training ML models.\n",
      "                        - Model training: This involves training ML\n",
      "\n",
      "\n",
      " [Page number]\n",
      "3\n",
      "\n",
      "\n",
      " [Source: file_name]\n",
      "practitioners_guide_to_mlops_whitepaper\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "print(\"[Context]\")\n",
    "print(pdf_mp_summary[\"chunks\"].iloc[index])\n",
    "print(\"\\n\\n [Simple Summary]\")\n",
    "print(pdf_mp_summary[\"concise_summary\"].iloc[index])\n",
    "print(\"\\n\\n [Page number]\")\n",
    "print(pdf_mp_summary[\"page_number\"].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_mp_summary[\"file_name\"].iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROrE1-HKpg7y"
   },
   "source": [
    "### Considerations\n",
    "\n",
    "With `MapReduce` method, the model is able to summarize a large paper by overcoming the context limit of `Stuffing` method with parallel processing.\n",
    "\n",
    "However, the `MapReduce` requires multiple calls to the model and potentially losing context between pages.\n",
    "\n",
    "To deal this challenge, you can try another method to summarize multiple pages at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxdB-5PqgCf-"
   },
   "source": [
    "## Method 3: Refine\n",
    "\n",
    "The Refine method is an alternative method to deal with large document summarization. It works by first running an initial prompt on a small chunk of data, generating some output. Then, for each subsequent document, the output from the previous document is passed in along with the new document, and the LLM is asked to refine the output based on the new document.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the load_summarize_chain method. What you need to do is setting `refine` as `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjj2UZilDF4Q"
   },
   "source": [
    "### Prompt design with `Refine` chain\n",
    "\n",
    "With LangChain, the `refine` chain requires two prompts.\n",
    "\n",
    "The question prompt to generate the output for subsequent task. The refine prompt to refine the output based on the generated content.\n",
    "\n",
    "In this example, the question prompt is:\n",
    "\n",
    "```\n",
    "Please provide a summary of the following text.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "```\n",
    "\n",
    "and the refine prompt is:\n",
    "\n",
    "```\n",
    "Write a concise summary of the following text delimited by triple backquotes.\n",
    "Return your response in bullet points which covers the key points of the text.\n",
    "```{text}```\n",
    "BULLET POINT SUMMARY:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XiZX45Z5VTwS"
   },
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "                  Please provide a summary of the following text.\n",
    "                  TEXT: {text}\n",
    "                  SUMMARY:\n",
    "                  \"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "              \"\"\"\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-USlaSPbM0rs"
   },
   "source": [
    "### Generate summaries using Refine method\n",
    "\n",
    "After you define prompts, you initiate a summarization chain using `refine` chain type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_-Sv3HO1U3hi"
   },
   "outputs": [],
   "source": [
    "refine_chain = load_summarize_chain(\n",
    "    vertex_llm_text,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=question_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9EZCDK-MQJH"
   },
   "source": [
    "Then, you use the summatization chain to summarize document using Refine method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KHwwab7vXNa1"
   },
   "outputs": [],
   "source": [
    "refine_outputs = refine_chain({\"input_documents\": pages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUqpki5EMYEr"
   },
   "source": [
    "Below you can see the resulting summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7j5cUGStZ5WF"
   },
   "outputs": [],
   "source": [
    "final_refine_data = []\n",
    "for doc, out in zip(\n",
    "    refine_outputs[\"input_documents\"], refine_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_refine_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "N_7Mm9cEmGOV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunks</th>\n",
       "      <th>concise_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Practitioners guide to MLOps:  \\nA framework f...</td>\n",
       "      <td>This white paper provides a guide for practiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>Table of Contents\\nExecutive summary  3\\nOverv...</td>\n",
       "      <td>This document provides an overview of MLOps, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>Executive summary\\nAcross industries, DevOps a...</td>\n",
       "      <td>This document provides an overview of MLOps, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>4\\nOrganizations can use the framework to iden...</td>\n",
       "      <td>- MLOps is a framework that helps organization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>5\\nprocesses in place is one of the differenti...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 file_name file_type  page_number  \\\n",
       "0  practitioners_guide_to_mlops_whitepaper      .pdf            0   \n",
       "1  practitioners_guide_to_mlops_whitepaper      .pdf            1   \n",
       "2  practitioners_guide_to_mlops_whitepaper      .pdf            2   \n",
       "3  practitioners_guide_to_mlops_whitepaper      .pdf            3   \n",
       "4  practitioners_guide_to_mlops_whitepaper      .pdf            4   \n",
       "\n",
       "                                              chunks  \\\n",
       "0  Practitioners guide to MLOps:  \\nA framework f...   \n",
       "1  Table of Contents\\nExecutive summary  3\\nOverv...   \n",
       "2  Executive summary\\nAcross industries, DevOps a...   \n",
       "3  4\\nOrganizations can use the framework to iden...   \n",
       "4  5\\nprocesses in place is one of the differenti...   \n",
       "\n",
       "                                     concise_summary  \n",
       "0  This white paper provides a guide for practiti...  \n",
       "1  This document provides an overview of MLOps, i...  \n",
       "2  This document provides an overview of MLOps, w...  \n",
       "3  - MLOps is a framework that helps organization...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_refine_summary = pd.DataFrame.from_dict(final_refine_data)\n",
    "pdf_refine_summary = pdf_mp_summary.sort_values(\n",
    "    by=[\"file_name\", \"page_number\"]\n",
    ")  # sorting the datafram by filename and page_number\n",
    "pdf_refine_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_refine_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jvLVCs8Gbwbw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Context]\n",
      "4\n",
      "Organizations can use the framework to identify gaps in building an integrated ML platform and to focus on the scale \n",
      "and automate themes from Googleâ€™s AI Adoption Framework. The decision about whether (or to which degree) to \n",
      "adopt each of these processes and capabilities in your organization depends on your business context. For exam -\n",
      "ple, you must determine the business value that the framework creates when compared to the cost of purchasing or \n",
      "building capabilities (for example, the cost in engineering hours).\n",
      "Overview of MLOps lifecycle  \n",
      "and core capabilities\n",
      "Despite the growing recognition of AI/ML as a crucial pillar of digital transformation, successful deployments and \n",
      "effective operations are a bottleneck for getting value from AI. Only one in two organizations has moved beyond \n",
      "pilots and proofs of concept. Moreover, 72% of a cohort of organizations that began AI pilots before 2019 have not \n",
      "been able to deploy even a single application in production.1 Algorithmiaâ€™s survey of the state of enterprise machine \n",
      "learning found that 55% of companies surveyed have not deployed an ML model.2 To summarize: models donâ€™t make \n",
      "it into production, and if they do, they break because they fail to adapt to changes in the environment.\n",
      "This is due to a variety of issues. Teams engage in a high degree of manual and one-off work. They do not have reus -\n",
      "able or reproducible components, and their processes involve difficulties in handoffs between data scientists and IT. \n",
      "Deloitte identified lack of talent and integration issues as factors that can stall or derail AI initiatives.3 Algorithmiaâ€™s \n",
      "survey highlighted that challenges in deployment, scaling, and versioning efforts still hinder teams from getting value \n",
      "from their investments in ML. Capgemini Research noted that the top three challenges faced by organizations in \n",
      "achieving deployments at scale are lack of mid- to senior-level talent, lack of change-management processes, and \n",
      "lack of strong governance models for achieving scale.\n",
      "The common theme in these and other studies is that ML systems cannot be built in an ad hoc manner, isolated from \n",
      "other IT initiatives like DataOps and DevOps. They also cannot be built without adopting and applying sound software \n",
      "engineering practices, while taking into account the factors that make operationalizing ML different from operational -\n",
      "izing other types of software.\n",
      "Organizations need an automated and streamlined ML process. This process does not just help the organization \n",
      "successfully deploy ML models in production. It also helps manage risk when organizations scale the number of \n",
      "ML applications to more use cases in changing environments, and it helps ensure that the applications are still in \n",
      "line with business goals. McKinseyâ€™s Global Survey on AI found that having standard frameworks and development \n",
      "1 The AI-powered enterprise , CapGemini Research Institute, 2020.\n",
      "2 2020 state of enterprise machine learning , Algorithmia, 2020.\n",
      "3 Artificial intelligence for the real world , Deloitte, 2017.\n",
      "4 The state of AI in 2020 , McKinsey, 2020.\n",
      "\n",
      "\n",
      " [Simple Summary]\n",
      "- MLOps is a framework that helps organizations operationalize machine learning.\n",
      "                      - MLOps helps organizations to successfully deploy ML models in production, manage risk when scaling the number of ML applications, and ensure that the applications are still in line with business goals.\n",
      "                      - MLOps is made up of a set of processes and capabilities that cover the entire ML lifecycle, from data preparation to model deployment and monitoring.\n",
      "                      - The key processes and capabilities of MLOps include:\n",
      "                        - Data preparation: This involves cleaning and preparing data for training ML models.\n",
      "                        - Model training: This involves training ML\n",
      "\n",
      "\n",
      " [Page number]\n",
      "3\n",
      "\n",
      "\n",
      " [Source: file_name]\n",
      "practitioners_guide_to_mlops_whitepaper\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "print(\"[Context]\")\n",
    "print(pdf_refine_summary[\"chunks\"].iloc[index])\n",
    "print(\"\\n\\n [Simple Summary]\")\n",
    "print(pdf_refine_summary[\"concise_summary\"].iloc[index])\n",
    "print(\"\\n\\n [Page number]\")\n",
    "print(pdf_refine_summary[\"page_number\"].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_refine_summary[\"file_name\"].iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dwgbRTrM5Cb"
   },
   "source": [
    "### Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H0Y5pPcXbgm"
   },
   "source": [
    "In short, the Refine method for text summarization with LLMs can pull in more relevant context and may be less lossy than Map Reduce. However, it requires many more calls to the LLM than Stuffing, and these calls are not independent, meaning they cannot be parallelized. Additionally, there is some potential dependency on the ordering of the documents. Latest documents they might become more relevant as this method suffers from recency bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAaWXncPMhv4"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "In this notebook you learn about different techniques to summarize long documents with LangChain and PaLM API. What you have seen in this notebook are only some of the possibilities you have. For example, there is another method called the Map-Rerank method which involves running an initial prompt on each chunk of data, which not only tries to complete a task but also gives a score for how certain it is in its answer. The responses are then ranked according to this score, and the highest score is returned.\n",
    "\n",
    "With that being said, it is important to highlight that depending on your needs you may consider to use pure Foundational model with a custom framework to build generative ai application.\n",
    "\n",
    "Here are some of the benefits of using a foundational model with a custom framework:\n",
    "\n",
    " - More flexibility to implement your application with different LLMs, prompting templates, document handling strategies and more.\n",
    "\n",
    " - More control to customize your generative applications based on your scenario.\n",
    "\n",
    " - Better performance to improve latency and scalability of your application.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "summarization_large_documents_langchain.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-env-genai-py",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "Python 3 (genai)",
   "language": "python",
   "name": "conda-env-genai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
